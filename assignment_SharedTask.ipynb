{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modern-personal",
   "metadata": {},
   "source": [
    "# Early prediction of cart abandonement\n",
    "\n",
    "\n",
    "This shared task is a simplified version of the Coveo Data Challenge organised at the eCommerce workshop co-located with the Special Interest Group for Information Retrieval (SIGIR) Conference in July 2021 in Montreal. Your goal will be to perform early prediction about whether a user will abandon their cart in the current shopping session after at least a product has been added to the cart. This is therefore a binary classification problem: a session can either feature in the abandon category or in the purchase category.\n",
    "\n",
    "Your baseline model is a Naïve Bayes classifier of order 4, which thus predicts cart abandonement based on 4grams of symbolised clicks. No other piece of information should go into this model.\n",
    "\n",
    "Early prediction will be evaluated at 5, 10, and 15 clicks after the first add-to-cart event in a session. You should ignore sessions where no product is added to the cart while training your system.\n",
    "\n",
    "You will receive a training set (to be pre-processed) right away and use it to develop your pipeline - pre-process the data, derive a development set to test your model implementations and develop the code you need to complete the later parts of the assignment. You will also immediately receive a test set (already pre-processed), consisting of three .json files containing sessions trimmed after 5, 10, and 15 events post add to cart, with no information about which are purchase sessions and which aren't. Use these files to make sure your pipeline can make predictions using those as input. You need to return files with the predicted label for each session, as obtained by running the baseline model and your own model of choice. After the submission deadline, you will receive the true labels of the test sessions and you will have to send in a final file with the oracle prediction for the test sessions. The baseline model needs to be trained on the whole training corpus you receive, with no re-sampling and no separate models trained to predict for 5, 10, 15 events post add to cart. The experimental model can be trained following any strategy you observe to yield better results. The oracle needs to be estimated on the full test sets, one oracle for 5-event sessions, one for 10-event sessions, one for 15-event sessions.\n",
    "\n",
    "You should carry out the following tasks:\n",
    "1. pre-process the sessions in the training set (you can execute these tasks in the order you prefer as long as it makes logical sense, but make sure to specify which task is being solved in which block of code):\n",
    "    1. sessionise <font color='red'>(1pt)</font>\n",
    "    2. select sessions with at least one add-to-cart and discard all the others <font color='red'>(1pt)</font>\n",
    "    3. add class labels: treat purchase as the positive class <font color='red'>(1pt)</font>\n",
    "    4. cut purchase sessions to the last event before the first purchase <font color='red'>(1pt)</font>\n",
    "    5. remove sessions shorter than 5 and longer than 155 clicks <font color='red'>(1pt)</font>\n",
    "    6. symbolise actions (assign numerical values according to event frequency, there should be for symbols: view, detail, add, remove) <font color='red'>(1pt)</font>\n",
    "\n",
    "2. implement the baseline model defined above (Naïve Bayes, order 4) and send in the predictions for the sessions in the test set (as three .json files, one for sessions with 5-events post add to cart, one for sessions with 10 events post add to cart, one for sessions with 15 events post add to cart, see formatting requirements below). I will compare the predictions with my own implementation and award points if the F1 score of your predictions and mine match for sessions of 5, 10, and 15 events (2 points for each matching F1).\n",
    "\n",
    "3. implement another model of your choice (an SVM, a Markov Chain, a neural network, an anomaly detection algorithm if you feel more adventurous, or something else) and send in the predictions for the sessions in the test set, again as three separate .json files, one for each early prediction task (at 5, 10, 15 events post add to cart).\n",
    "\n",
    "You are not allowed to submit another Naïve Bayes Classifier where you only changed the ngram size as an experimental model, but you can feed your model any information you can retrieve from the dataset, including dwell times, day-of-the-week, time of the day, and SKUs. Therefore, a NBC fed with more than just clicks is a valid experimental model. However, you are not allowed to use additional data sources other than the training set: your notebook has to run considering only the information provided in the file you downloaded from the WeTransfer link. One week prior to the deadline I will share the test sessions, already pre-processed. You should run your trained models on such sessions and return files with the predicted label for each session. I will compare your predictions to the real labels and compute an f1 score, separately for sessions of 5, 10, and 15 events <font color='red'>(10 pts attributed to the model with the best f1 at 15 clicks post add-to-cart. Then, the difference in f1 between the baseline and the best model will be partitioned in ten bins of equal size. Models falling in the top bin will get 9 pts, models falling in the second bin will get 8.5 pts and so on. Models falling in the lowest bin (closest to baseline) will get 4.5 pts while working models not outperforming the baseline will get 2 pts. Even if I only consider performance after 15 events, you need to also submit predictions for sessions of 5 and 10 events, or no points will be awarded. If you don't submit predictions, no points will be awarded, even if the submitted notebook contains (working) code.</font>\n",
    "    \n",
    "4. implement an oracle model to get the upper bound on performance and send in its predictions for the sessions in the test set, always as three separate .json files, one for each early prediction task (at 5, 10, 15 events post add to cart). I will compare the predictions with my own implementation and award points if the F1 score matches for sessions of 5, 10, and 15 events post add to cart (1 point for each matching F1).\n",
    "\n",
    "At the end of these four tasks you will thus have submitted 13 different files:\n",
    "* the python notebook (naming convention: 'AoCD_assignment_group[groupID].ipynb')\n",
    "* the baseline predictions (naming convention: 'predictions_group[groupID]_baseline_at{5|10|15}.json')\n",
    "* the experimental model predictions (naming convention: 'predictions_group[groupID]_expModel_at{5|10|15}.json')\n",
    "* the oracle predictions (naming convention: 'predictions_group[groupID]_oracle_at{5|10|15}.json')\n",
    "\n",
    "Each prediction file should contain a simple Python dictionary mapping session IDs (keys, as strings - those found in the test files I make available) to predictions (values, integers, either 0 or 1). Failing to adhere to this format will result in no points being awarded.\n",
    "\n",
    "5. do error analysis on one of the models (can be the baseline or your own model, or a comparison between their predictions and effectiveness) (<font color='red'>5 pts: you can use any approach, you will get points based on the conclusions you draw from the error analysis, there isn't a right and wrong here, but sensible and not). This should feature in the notebook: use the dev set as your eval data: in real life you should do this on the test set, but since the test set complete with true labels will be released late to ensure a fair evaluation procedure, you should inspect your models' behavior using the dev set.</font>\n",
    "\n",
    "Make sure to indicate clearly in the notebook which output matches a certain task, using the task ID. No points will be awarded if you fail to indicate which task a certain code-block or markdown cell is addressing.\n",
    "\n",
    "You should submit a complete notebook. If dependencies are required to run your code, make sure to install them at the beginning of the notebook. The only input files should be the ones you receive from me. Make sure you comment the code explaining what happens (use doc-strings to define what functions do, inline comments for more detailed information inside functions, as well as markdown cells to highlight the flow). If you submit undocumented code, you will automatically get a 1/10.\n",
    "\n",
    "You need 17 points out of 30 to pass, meaning that 17/30 translates to a 5.5/10. In principle you can pass if you submit predictions from a model which doesn't beat the baseline, but you have to do tasks 1, 2, and 4 perfectly. We have seen how to pre-process the data, build an oracle model and how to implement a Naive Bayes Classifier during the practicals, so you have a blueprint for the first three tasks. If you stop here, however, you won't pass the assignment, so you have to engage with the implementation of a new model or feed new info to the baseline architecture. If you want to get a grade higher than 8 or edge your chances to pass, you have to engage with the error analysis.\n",
    "\n",
    "IMPORTANT: I will consider predictions from one experimental model per group. You might have to test more than one (so derive a dev set from the training data to test different models), but only submit the one you think works best!! You can submit multiple times, but I'll consider the last file you submit (the last baseline predictions, the last experimental model predictions, the last oracle predictions, the last notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here: feel free to organise it as you prefer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-health",
   "metadata": {},
   "source": [
    "Remember to <font color='red'>comment the error analysis</font>! You can use this markdown cell to do it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b03db6c5b70bf2dce4acc35539e0b3352e79d13b523ff09bdee52816e267e368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
